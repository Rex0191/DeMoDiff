<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Spatiotemporally Decoupled Autoregressive Diffusion Model for Text-driven Human Motion Generation.">
  <meta name="keywords" content="Text-driven human motion generation, Autoregressive diffusion model, Spatial-temporal decoupling">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Spatiotemporally Decoupled Autoregressive Diffusion Model for Text-driven Human Motion Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/demodiff.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://liangxuy.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          <!-- </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://liangxuy.github.io/actformer">
              ActFormer
            </a>
            <a class="navbar-item" href="https://liangxuy.github.io/ReGenNet">
              ReGenNet
            </a>
            <a class="navbar-item" href="https://liangxuy.github.io/inter-x">
              Inter-X
            </a>
            <a class="navbar-item" href="https://github.com/LvXinTao/himo_dataset">
              HIMO
            </a>
            <a class="navbar-item" href="https://liangxuy.github.io/InterVLA">
              InterVLA -->
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> 


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Spatiotemporally Decoupled Autoregressive Diffusion Model for Text-driven Human Motion Generation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Chengqun Yang,</span>
              <span class="author-block">
                <a href="https://liangxuy.github.io/">Liang Xu</a>,</span>
              
              <span class="author-block">
                Yanping Li, Fulong Liu,</span>
              <span class="author-block">
                <a href="https://g-1nonly.github.io/">Jingnan Gao</a>,</span>
              </span>
              <span class="author-block">
                Weili Zeng,</span>
              <span class="author-block">
                <a href="https://daodaofr.github.io/">Yichao Yan*</a>,
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University.</span>
  
              

            </div>
            <div>( * denotes Corresponding author. )</div>
            <!-- <div class="is-size-3">TMM 2025 Submission</div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2203.07706.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2203.07706" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->

                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/Szy-Young/actformer"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/Szy-Young/actformer"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/teaser.png" class="interpolation-image" alt="Interpolate start reference image." />
        <h2 class="subtitle has-text-centered">
          DeMoDiff is composed of spatial-temporal VAE and spatial-temporal autoregressive diffusion model for enhanced motion representation, generation, and editing capabilities.
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Text-driven human motion synthesis has made substantial development with two core modules of motion representation and generative architecture. 
              For representation, Vector Quantization (VQ)-based methods compress motion data into discrete tokens while latent-based models operate directly in continuous space.
              However, both of these representations exhibit significant limitations.
              VQ-based methods suffer from inherent information loss, which compromises the quality, diversity, and generalization of generated motions, while continuous representation on holistic whole-body motion hinders part-level flexibility.
              For architecture, diffusion and autoregressive diffusion models have demonstrated their superiority, yet the fine-grained controllability over individual body parts is also limited.
            </p>
            <p>
              To overcome the aforementioned issues, we propose a unified spatiotemporally decoupled framework named DeMoDiff, which jointly redesigns representation and architecture.
              First, we propose a lightweight spatial-temporal VAE that encodes each body joint rather than compressing the whole-body motion into a single latent space.
              The decoupled paradigm substantially enhances representation extraction capabilities and offers greater part-level controllability.
              Second, we incorporate spatial-temporal masking and attention mechanisms into an autoregressive diffusion generator, achieving both generative capability and controllable editability.

            </p>
            <p>
              Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that our model achieves state-of-the-art reconstruction performance and compelling motion generation results.
              Moreover, our framework demonstrates strong temporal and spatial editing capabilities, further validating its effectiveness.

            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Method</h2>
          <div class="content has-text-centered">
            <img src="./static/images/network.png" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>
          <div class="content has-text-justified">
            <p>
              (A) Spatial-Temporal VAE encodes motion into 2D latent and decodes it for reconstruction. (B) Spatial-Temporal Autoregressive Diffusion includes Spatial-Temporal Attention, Spatial Attention and Temporal Attention and is trained via mask modeling, using a diffusion head to predict motion latent. (C) During inference, all latent starts as masked latent. Spatial-Temporal Autoregressive Diffusion predicts the conditional latent, and then the diffusion head generates the predict latent.
            </p>
          </div>
        </div>
      </div>


      <!-- More visualization results. -->
      <style>
        
        .visual-grid {
          display: grid !important;
          grid-template-columns: repeat(3, 1fr) !important;
          gap: 24px !important;
          width: 100%;
          margin-top: 2rem;
        }
      
        .visual-card {
          background: #fafafa;
          border-radius: 8px;
          padding: 16px;
          box-shadow: 0 2px 4px rgba(10, 10, 10, 0.06);
          display: flex;
          flex-direction: column;
          height: 100%;
        }
      
        .visual-card video {
          width: 100%;
          height: 180px;
          object-fit: contain;
          background: #ffffff;
          border-radius: 6px;
        }
      
        .visual-card-title {
          margin-top: 12px;
          font-weight: 600;
          font-size: 0.95rem;
        }
      
        .visual-card-text {
          margin-top: 6px;
          font-size: 0.95rem;
          line-height: 0.85;
          color: #4b5563;
        }
      
        /* 移动端自动变回单列 */
        @media (max-width: 768px) {
          .visual-grid {
            grid-template-columns: 1fr !important;
          }
        }
      </style>
      
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Visualization results</h2>
      
          <div class="visual-grid">
            <!-- 卡片 1 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/18_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Pick up an object from the ground</div> -->
              <div class="visual-card-text">
                A person walks turning to the left.
              </div>
            </div>
      
            <!-- 卡片 2 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/14_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Throw with dominant hand</div> -->
              <div class="visual-card-text">
                A person slowly walks forward.
              </div>
            </div>
      
            <!-- 卡片 3 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/15_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Side kick with balance adjustment</div> -->
              <div class="visual-card-text">
                The man takes backwards.
              </div>
            </div>
      
            <!-- 卡片 4 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/16_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Wave hand repeatedly</div> -->
              <div class="visual-card-text">
                A person waves both arms above head.
              </div>
            </div>
      
            <!-- 卡片 5 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/25_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Salute with upper-body alignment</div> -->
              <div class="visual-card-text">
                The person steps forward and uses the left leg to kick something forward.
              </div>
            </div>
      
            <!-- 卡片 6 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/19_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Full-body stretching</div> -->
              <div class="visual-card-text">
                A man uses his left hand to throw something with force.
              </div>
            </div>
      
            <!-- 卡片 7 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/28_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Rhythmic dance movement</div> -->
              <div class="visual-card-text">
                A person walks forward and raises their arms in victory.
              </div>
            </div>
      
            <!-- 卡片 8 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/17_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Touch an object on the side</div> -->
              <div class="visual-card-text">
                A person steps to the left sideways.
              </div>
            </div>
      
            <!-- 卡片 9 -->
            <div class="visual-card">
              <video playsinline autoplay loop preload muted>
                <source src="./static/videos/21_demo.mp4" type="video/mp4">
              </video>
              <!-- <div class="visual-card-title">Swing a single body part</div> -->
              <div class="visual-card-text">
                A person is walking on a circle.
              </div>
            </div>
          </div>
        </div>
      </div>
      
  </section>


  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xu2023actformer,
      author    = {Xu, Liang and Song, Ziyang and Wang, Dongliang and Su, Jing and Fang, Zhicheng and Ding, Chenjing and Gan, Weihao and Yan, Yichao and Jin, Xin and Yang, Xiaokang and Zeng, Wenjun and Wu, Wei},
      title     = {ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation},
      journal   = {ICCV},
      year      = {2023},
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Thanks for the template from <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>